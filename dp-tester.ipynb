{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1270b5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:35.670601Z",
     "iopub.status.busy": "2024-01-29T19:52:35.670299Z",
     "iopub.status.idle": "2024-01-29T19:52:35.835940Z",
     "shell.execute_reply": "2024-01-29T19:52:35.835071Z"
    },
    "papermill": {
     "duration": 0.174161,
     "end_time": "2024-01-29T19:52:35.837848",
     "exception": false,
     "start_time": "2024-01-29T19:52:35.663687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "import sys\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tempfile\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "from IPython.display import display, Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99933685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:35.849548Z",
     "iopub.status.busy": "2024-01-29T19:52:35.849250Z",
     "iopub.status.idle": "2024-01-29T19:52:35.853383Z",
     "shell.execute_reply": "2024-01-29T19:52:35.852586Z"
    },
    "papermill": {
     "duration": 0.012291,
     "end_time": "2024-01-29T19:52:35.855591",
     "exception": false,
     "start_time": "2024-01-29T19:52:35.843300",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:13.143058158Z",
     "start_time": "2024-02-09T13:50:13.141289840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michal/Documents/FIT/DP/dp\n"
     ]
    }
   ],
   "source": [
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4948567f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:35.867403Z",
     "iopub.status.busy": "2024-01-29T19:52:35.867134Z",
     "iopub.status.idle": "2024-01-29T19:52:35.870833Z",
     "shell.execute_reply": "2024-01-29T19:52:35.869951Z"
    },
    "papermill": {
     "duration": 0.012341,
     "end_time": "2024-01-29T19:52:35.873281",
     "exception": false,
     "start_time": "2024-01-29T19:52:35.860940",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:13.144053877Z",
     "start_time": "2024-02-09T13:50:13.141923316Z"
    }
   },
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"/home/michal/Documents/FIT/DP/dp/src/data/input/DSCN0003.MP4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a481e304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:35.885223Z",
     "iopub.status.busy": "2024-01-29T19:52:35.884959Z",
     "iopub.status.idle": "2024-01-29T19:52:50.034220Z",
     "shell.execute_reply": "2024-01-29T19:52:50.033257Z"
    },
    "papermill": {
     "duration": 14.157567,
     "end_time": "2024-01-29T19:52:50.036661",
     "exception": false,
     "start_time": "2024-01-29T19:52:35.879094",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:14.906929162Z",
     "start_time": "2024-02-09T13:50:13.142265782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.8/dist-packages (8.0.239)\r\n",
      "Requirement already satisfied: psutil in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (5.9.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (3.7.4)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (10.2.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (4.9.0.80)\r\n",
      "Requirement already satisfied: numpy>=1.22.2 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (1.23.5)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (2.2.0)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: hub-sdk>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.0.3)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.31.0)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (1.10.1)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (0.17.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (4.66.1)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/michal/.local/lib/python3.8/site-packages (from ultralytics) (1.5.2)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.1.1.post2209072238)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from ultralytics) (5.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (5.10.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (4.38.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/michal/.local/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\r\n",
      "Requirement already satisfied: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (2.2.0)\r\n",
      "Requirement already satisfied: sympy in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (3.1)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\r\n",
      "Requirement already satisfied: fsspec in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->ultralytics) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/michal/.local/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.23.0->ultralytics) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.23.0->ultralytics) (1.25.8)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.23.0->ultralytics) (2.8)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/michal/.local/lib/python3.8/site-packages (from pandas>=1.1.4->ultralytics) (2022.6)\r\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/michal/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib>=3.3.0->ultralytics) (3.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.14.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/michal/.local/lib/python3.8/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/michal/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.8.0->ultralytics) (12.3.101)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/michal/.local/lib/python3.8/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89507d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:50.052435Z",
     "iopub.status.busy": "2024-01-29T19:52:50.051886Z",
     "iopub.status.idle": "2024-01-29T19:52:56.454277Z",
     "shell.execute_reply": "2024-01-29T19:52:56.453336Z"
    },
    "papermill": {
     "duration": 6.41257,
     "end_time": "2024-01-29T19:52:56.456201",
     "exception": false,
     "start_time": "2024-01-29T19:52:50.043631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.239 🚀 Python-3.8.10 torch-2.2.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Ti, 16075MiB)\n",
      "Setup complete ✅ (12 CPUs, 31.1 GB RAM, 226.9/915.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b115e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:56.471237Z",
     "iopub.status.busy": "2024-01-29T19:52:56.470801Z",
     "iopub.status.idle": "2024-01-29T19:52:58.419340Z",
     "shell.execute_reply": "2024-01-29T19:52:58.418293Z"
    },
    "papermill": {
     "duration": 1.958201,
     "end_time": "2024-01-29T19:52:58.421232",
     "exception": false,
     "start_time": "2024-01-29T19:52:56.463031",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:16.818209388Z",
     "start_time": "2024-02-09T13:50:16.727099464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate YOLOv8 model\n",
    "\n",
    "model = YOLO(f'/home/michal/Documents/FIT/DP/dp/src/impl/weights/yolov8n.pt')\n",
    "colors = np.random.randint(0, 256, size=(len(model.names), 3))\n",
    "\n",
    "print(model.names)\n",
    "\n",
    "# Specify which classes you care about. The rest of classes will be filtered out.\n",
    "chosen_class_ids = [0] # person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8965eaa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:58.437299Z",
     "iopub.status.busy": "2024-01-29T19:52:58.437016Z",
     "iopub.status.idle": "2024-01-29T19:52:58.455638Z",
     "shell.execute_reply": "2024-01-29T19:52:58.454854Z"
    },
    "papermill": {
     "duration": 0.028592,
     "end_time": "2024-01-29T19:52:58.457459",
     "exception": false,
     "start_time": "2024-01-29T19:52:58.428867",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:16.818439903Z",
     "start_time": "2024-02-09T13:50:16.816998322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fb539d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:52:58.473094Z",
     "iopub.status.busy": "2024-01-29T19:52:58.472799Z",
     "iopub.status.idle": "2024-01-29T19:53:17.023372Z",
     "shell.execute_reply": "2024-01-29T19:53:17.022325Z"
    },
    "papermill": {
     "duration": 18.561064,
     "end_time": "2024-01-29T19:53:17.025864",
     "exception": false,
     "start_time": "2024-01-29T19:52:58.464800",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:20.108604268Z",
     "start_time": "2024-02-09T13:50:16.817349926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michal/Documents/FIT/DP/dp/weights\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir {HOME}/weights\n",
    "%cd {HOME}/weights\n",
    "\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aaae12f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:17.043067Z",
     "iopub.status.busy": "2024-01-29T19:53:17.042387Z",
     "iopub.status.idle": "2024-01-29T19:53:25.229193Z",
     "shell.execute_reply": "2024-01-29T19:53:25.228397Z"
    },
    "papermill": {
     "duration": 8.197826,
     "end_time": "2024-01-29T19:53:25.231515",
     "exception": false,
     "start_time": "2024-01-29T19:53:17.033689",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:24.432310947Z",
     "start_time": "2024-02-09T13:50:20.113741835Z"
    }
   },
   "outputs": [],
   "source": [
    "sam_model = \"/home/michal/Documents/FIT/DP/dp/src/impl/weights/sam_vit_h_4b8939.pth\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_model).to(device=DEVICE)\n",
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339e55fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:27.011471Z",
     "iopub.status.busy": "2024-01-29T19:53:27.010812Z",
     "iopub.status.idle": "2024-01-29T19:53:27.018037Z",
     "shell.execute_reply": "2024-01-29T19:53:27.016998Z"
    },
    "papermill": {
     "duration": 0.020955,
     "end_time": "2024-01-29T19:53:27.022186",
     "exception": false,
     "start_time": "2024-01-29T19:53:27.001231",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:24.438712440Z",
     "start_time": "2024-02-09T13:50:24.436110409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05007ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:27.289372Z",
     "iopub.status.busy": "2024-01-29T19:53:27.288492Z",
     "iopub.status.idle": "2024-01-29T19:53:27.314825Z",
     "shell.execute_reply": "2024-01-29T19:53:27.313770Z"
    },
    "papermill": {
     "duration": 0.267267,
     "end_time": "2024-01-29T19:53:27.317090",
     "exception": false,
     "start_time": "2024-01-29T19:53:27.049823",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:24.489397810Z",
     "start_time": "2024-02-09T13:50:24.445001420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cast color to ints\n",
    "def get_color(color):\n",
    "    return (int(color[0]), int(color[1]), int(color[2]))\n",
    "\n",
    "# Get video dimensions\n",
    "def get_video_dimensions(input_cap):\n",
    "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return width, height\n",
    "def get_videoFps(cap):\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    return fps\n",
    "# Get output video writer with same dimensions and fps as input video\n",
    "def get_output_video_writer(input_cap, output_path):\n",
    "    # Get the video's properties (width, height, FPS)\n",
    "   \n",
    "    fps = int(input_cap.get(cv2.CAP_PROP_FPS))\n",
    "    width, height = get_video_dimensions(input_cap)\n",
    "    fps = get_videoFps(input_cap)\n",
    "  # Define the output video file\n",
    "    output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
    "    output_video = cv2.VideoWriter(output_path, output_codec, fps, (width, height))\n",
    "\n",
    "    return output_video\n",
    "\n",
    "# Visualize a video frame with bounding boxes, classes and confidence scores\n",
    "def visualize_detections(frame, boxes, conf_thresholds, class_ids):\n",
    "    frame_copy = np.copy(frame)\n",
    "    for idx in range(len(boxes)):\n",
    "        class_id = int(class_ids[idx])\n",
    "        conf = float(conf_thresholds[idx])\n",
    "        x1, y1, x2, y2 = int(boxes[idx][0]), int(boxes[idx][1]), int(boxes[idx][2]), int(boxes[idx][3])\n",
    "        color = colors[class_id]\n",
    "        label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
    "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), get_color(color), 2)\n",
    "        cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, get_color(color), 2)\n",
    "    return frame_copy\n",
    "\n",
    "def add_color_to_mask(mask, color):\n",
    "    next_mask = mask.astype(np.uint8)\n",
    "    next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
    "    next_mask = np.moveaxis(next_mask, 0, -1)\n",
    "    return next_mask * color\n",
    "\n",
    "# Merge masks into a single, multi-colored mask\n",
    "def merge_masks_colored(masks, class_ids):\n",
    "    filtered_class_ids = []\n",
    "    filtered_masks = []\n",
    "    for idx, cid in enumerate(class_ids):\n",
    "        if int(cid) in chosen_class_ids:\n",
    "            filtered_class_ids.append(cid)\n",
    "            filtered_masks.append(masks[idx])\n",
    "\n",
    "    merged_with_colors = add_color_to_mask(filtered_masks[0][0], get_color(colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
    "\n",
    "    if len(filtered_masks) == 1:\n",
    "        return merged_with_colors\n",
    "\n",
    "    for i in range(1, len(filtered_masks)):\n",
    "        curr_mask_with_colors = add_color_to_mask(filtered_masks[i][0], get_color(colors[int(filtered_class_ids[i])]))\n",
    "        merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n",
    "\n",
    "    return merged_with_colors.astype(np.uint8)\n",
    "\n",
    "def get_instance_uri(client, global_key, array):\n",
    "    \"\"\" Reads a numpy array into a temp Labelbox data row to-be-uploaded to Labelbox\n",
    "    Args:\n",
    "        client        :   Required (lb.Client) - Labelbox Client object\n",
    "        global_key    :   Required (str) - Data row global key\n",
    "        array         :   Required (np.ndarray) - NumPy ndarray representation of an image\n",
    "    Returns:\n",
    "        Temp Labelbox data row to-be-uploaded to Labelbox as row data\n",
    "    \"\"\"\n",
    "    # Convert array to PIL image\n",
    "    image_as_pil = PIL.Image.fromarray(array)\n",
    "    # Convert PIL image to PNG file bytes\n",
    "    image_as_bytes = BytesIO()\n",
    "    image_as_pil.save(image_as_bytes, format='PNG')\n",
    "    image_as_bytes = image_as_bytes.getvalue()\n",
    "    # Convert PNG file bytes to a temporary Labelbox URL\n",
    "    url = client.upload_data(\n",
    "        content=image_as_bytes, \n",
    "        filename=global_key,\n",
    "        content_type=\"image/jpeg\",\n",
    "        sign=True\n",
    "    )\n",
    "    # Return the URL\n",
    "    return url\n",
    "\n",
    "def get_local_instance_uri(array):\n",
    "    # Convert array to PIL image\n",
    "    image_as_pil = PIL.Image.fromarray(array)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix='.png', dir=\"/content\", delete=False) as temp_file:\n",
    "        image_as_pil.save(temp_file)\n",
    "        file_name = temp_file.name\n",
    "\n",
    "    # Return the URL\n",
    "    return file_name\n",
    "\n",
    "def create_mask_frame(frame_num, instance_uri):\n",
    "    return lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n",
    "\n",
    "def create_mask_instances(class_ids):\n",
    "    instances = []\n",
    "    for cid in list(set(class_ids)): # get unique class ids\n",
    "        if int(cid) in chosen_class_ids:\n",
    "            color = get_color(colors[int(cid)])\n",
    "            name = model.names[int(cid)]\n",
    "            instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n",
    "    return instances\n",
    "\n",
    "def create_video_mask_annotation(frames, instance):\n",
    "    return lb_types.VideoMaskAnnotation(\n",
    "        frames=frames,\n",
    "        instances=[instance]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c27daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:27.374877Z",
     "iopub.status.busy": "2024-01-29T19:53:27.374389Z",
     "iopub.status.idle": "2024-01-29T19:53:39.602301Z",
     "shell.execute_reply": "2024-01-29T19:53:39.601574Z"
    },
    "papermill": {
     "duration": 12.277002,
     "end_time": "2024-01-29T19:53:39.604180",
     "exception": false,
     "start_time": "2024-01-29T19:53:27.327178",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:28.139730262Z",
     "start_time": "2024-02-09T13:50:26.697426423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.6ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 1\n",
      "\n",
      "0: 384x640 1 person, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 2.6ms preprocess, 6.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.6ms\n",
      "Speed: 2.2ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 2.6ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 2.6ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.3ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 2.5ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 2.9ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 2.3ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 90\n",
      "0: 384x640 2 persons, 4.7ms\n",
      "Speed: 2.1ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 2.4ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 2.4ms preprocess, 6.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 persons, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processing frame number 120\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 1 person, 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run through YOLOv8 on the video once quickly to get unique class ids present\n",
    "# This will inform which classes we add to the ontology\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "unique_class_ids = set()\n",
    "\n",
    "# Loop through the frames of the video\n",
    "frame_num = 1\n",
    "while cap.isOpened():\n",
    "    if frame_num % 30 == 0 or frame_num == 1:\n",
    "        print(\"Processing frame number\", frame_num)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "  # Run frame through YOLOv8 and get class ids predicted\n",
    "    detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
    "    for cid in detections[0].boxes.cls:\n",
    "        unique_class_ids.add(int(cid))\n",
    "        frame_num += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30cfb6",
   "metadata": {
    "papermill": {
     "duration": 0.177749,
     "end_time": "2024-01-29T19:53:40.000855",
     "exception": false,
     "start_time": "2024-01-29T19:53:39.823106",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a007b5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:40.385348Z",
     "iopub.status.busy": "2024-01-29T19:53:40.384998Z",
     "iopub.status.idle": "2024-01-29T19:53:40.390767Z",
     "shell.execute_reply": "2024-01-29T19:53:40.389846Z"
    },
    "papermill": {
     "duration": 0.213098,
     "end_time": "2024-01-29T19:53:40.392722",
     "exception": false,
     "start_time": "2024-01-29T19:53:40.179624",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:50:29.963361160Z",
     "start_time": "2024-02-09T13:50:29.958210097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/michal/Documents/FIT/DP/dp/src/data/input/DSCN0003.MP4'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eebdc961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:53:40.750009Z",
     "iopub.status.busy": "2024-01-29T19:53:40.749103Z",
     "iopub.status.idle": "2024-01-29T19:54:29.952997Z",
     "shell.execute_reply": "2024-01-29T19:54:29.952174Z"
    },
    "papermill": {
     "duration": 49.385116,
     "end_time": "2024-01-29T19:54:29.955230",
     "exception": false,
     "start_time": "2024-01-29T19:53:40.570114",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T13:53:06.024649741Z",
     "start_time": "2024-02-09T13:51:40.645776115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1 - 30\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 9.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1\n",
      "No boxes found on frame 1\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2\n",
      "No boxes found on frame 2\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "3\n",
      "No boxes found on frame 3\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "4\n",
      "No boxes found on frame 4\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "5\n",
      "No boxes found on frame 5\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "6\n",
      "No boxes found on frame 6\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "7\n",
      "No boxes found on frame 7\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "8\n",
      "No boxes found on frame 8\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "9\n",
      "No boxes found on frame 9\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "10\n",
      "No boxes found on frame 10\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "11\n",
      "No boxes found on frame 11\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "12\n",
      "No boxes found on frame 12\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "13\n",
      "No boxes found on frame 13\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "14\n",
      "No boxes found on frame 14\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "15\n",
      "No boxes found on frame 15\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 4.3ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "16\n",
      "No boxes found on frame 16\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "17\n",
      "No boxes found on frame 17\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "18\n",
      "No boxes found on frame 18\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "19\n",
      "No boxes found on frame 19\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "20\n",
      "No boxes found on frame 20\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.0ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "21\n",
      "No boxes found on frame 21\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "22\n",
      "No boxes found on frame 22\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "23\n",
      "No boxes found on frame 23\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 3.0ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "24\n",
      "No boxes found on frame 24\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "25\n",
      "No boxes found on frame 25\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "26\n",
      "No boxes found on frame 26\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "27\n",
      "No boxes found on frame 27\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "28\n",
      "No boxes found on frame 28\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "29\n",
      "No boxes found on frame 29\n",
      "Processing frames 30 - 59\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "30\n",
      "No boxes found on frame 30\n",
      "\n",
      "0: 384x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "31\n",
      "0: 384x640 1 person, 7.0ms\n",
      "Speed: 2.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "32\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 2.4ms preprocess, 8.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "33\n",
      "\n",
      "0: 384x640 1 person, 5.7ms\n",
      "Speed: 2.9ms preprocess, 5.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "34\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 4.4ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "35\n",
      "\n",
      "0: 384x640 1 person, 5.3ms\n",
      "Speed: 3.4ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "36\n",
      "\n",
      "0: 384x640 1 person, 5.8ms\n",
      "Speed: 3.6ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [15], line 64\u001B[0m\n\u001B[1;32m     61\u001B[0m   cv2\u001B[38;5;241m.\u001B[39mresizeWindow(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mframe_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1600\u001B[39m, \u001B[38;5;241m1200\u001B[39m)\n\u001B[1;32m     63\u001B[0m   cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mframe_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, image_combined)\n\u001B[0;32m---> 64\u001B[0m   \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# For the purposes of this demo, only look at the first 90 frames\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m frame_num \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m50\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Run YOLOv8 and then SAM on each frame, and write visualization videos to disk\n",
    "# You can download /content/skateboarding_boxes.mp4 and /content/skateboarding_masks.mp4\n",
    "# to visualize the results\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "output_video_boxes = get_output_video_writer(cap, \"/home/michal/Documents/FIT/DP/dp/src/data/output/tmp_boxes.mp4\")\n",
    "output_video_masks = get_output_video_writer(cap, \"/home/michal/Documents/FIT/DP/dp/src/data/output/tmp_masks.mp4\")\n",
    "mask_frames = []\n",
    "# print(output_video_boxes)\n",
    "# Loop through the frames of the video\n",
    "frame_num = 1\n",
    "while cap.isOpened():\n",
    "    if frame_num % 30 == 0 or frame_num == 1:\n",
    "        print(\"Processing frames\", frame_num, \"-\", frame_num+29)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "  # Run frame through YOLOv8 to get detections\n",
    "    detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
    "  \n",
    "  # Write detections to output video\n",
    "    frame_with_detections = visualize_detections(frame, \n",
    "                                                 detections[0].boxes.cpu().xyxy, \n",
    "                                                 detections[0].boxes.cpu().conf, \n",
    "                                                 detections[0].boxes.cpu().cls)\n",
    "    print(frame_num)\n",
    "    output_video_boxes.write(frame_with_detections)\n",
    "    \n",
    "    \n",
    "    # Run frame and detections through SAM to get masks\n",
    "    transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n",
    "    if len(transformed_boxes) == 0:\n",
    "        print(\"No boxes found on frame\", frame_num)\n",
    "        output_video_masks.write(frame)\n",
    "        frame_num += 1\n",
    "        continue\n",
    "    mask_predictor.set_image(frame)\n",
    "    masks, scores, logits = mask_predictor.predict_torch(\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output=False,\n",
    "        point_coords=None,\n",
    "        point_labels=None\n",
    "    )\n",
    "    masks = np.array(masks.cpu())\n",
    "    if masks is None or len(masks) == 0:\n",
    "        print(\"No masks found on frame\", frame_num)\n",
    "        output_video_masks.write(frame)\n",
    "        frame_num += 1\n",
    "        continue\n",
    "    merged_colored_mask = merge_masks_colored(masks, detections[0].boxes.cls)\n",
    "  \n",
    "  # Write masks to output video\n",
    "    image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
    "    output_video_masks.write(image_combined)\n",
    "    \n",
    "    frame_num += 1\n",
    "    cv2.namedWindow(f\"{frame_num}\", cv2.WINDOW_NORMAL)\n",
    "            # Using resizeWindow()\n",
    "    cv2.resizeWindow(f\"{frame_num}\", 1600, 1200)\n",
    "\n",
    "    cv2.imshow(f\"{frame_num}\", image_combined)\n",
    "    cv2.waitKey(0)\n",
    "  # For the purposes of this demo, only look at the first 90 frames\n",
    "    if frame_num > 50:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "output_video_boxes.release()\n",
    "output_video_masks.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa3b150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T19:54:30.365745Z",
     "iopub.status.busy": "2024-01-29T19:54:30.365065Z",
     "iopub.status.idle": "2024-01-29T19:54:30.370812Z",
     "shell.execute_reply": "2024-01-29T19:54:30.369947Z"
    },
    "papermill": {
     "duration": 0.212879,
     "end_time": "2024-01-29T19:54:30.372686",
     "exception": false,
     "start_time": "2024-01-29T19:54:30.159807",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-09T01:36:41.928189940Z",
     "start_time": "2024-02-09T01:36:41.925912821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/michal/Documents/FIT/DP/dp/src/data/input/DSCN0003.MP4'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886041e",
   "metadata": {
    "papermill": {
     "duration": 0.203491,
     "end_time": "2024-01-29T19:54:30.778227",
     "exception": false,
     "start_time": "2024-01-29T19:54:30.574736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26b1db",
   "metadata": {
    "papermill": {
     "duration": 0.203162,
     "end_time": "2024-01-29T19:54:31.212572",
     "exception": false,
     "start_time": "2024-01-29T19:54:31.009410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4372779,
     "sourceId": 7508206,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 120.305609,
   "end_time": "2024-01-29T19:54:32.739614",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-29T19:52:32.434005",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
