{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedfb27c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:13.175608Z",
     "iopub.status.busy": "2024-02-06T20:53:13.175260Z",
     "iopub.status.idle": "2024-02-06T20:53:13.952050Z",
     "shell.execute_reply": "2024-02-06T20:53:13.951194Z"
    },
    "papermill": {
     "duration": 0.784396,
     "end_time": "2024-02-06T20:53:13.954357",
     "exception": false,
     "start_time": "2024-02-06T20:53:13.169961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/input-data/skateboarding.mp4\n",
      "/kaggle/input/input-data/people_02.mp4\n",
      "/kaggle/input/input-data/people_01.mp4\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d011f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:13.962622Z",
     "iopub.status.busy": "2024-02-06T20:53:13.962259Z",
     "iopub.status.idle": "2024-02-06T20:53:21.893079Z",
     "shell.execute_reply": "2024-02-06T20:53:21.891139Z"
    },
    "papermill": {
     "duration": 7.937796,
     "end_time": "2024-02-06T20:53:21.895973",
     "exception": false,
     "start_time": "2024-02-06T20:53:13.958177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "!mkdir weights\n",
    "class SAM_handler:\n",
    "    def __init__(self, device):\n",
    "        HOME ='/kaggle/working'\n",
    "        if not os.path.exists(f\"/home/michal/Documents/FIT/DP/dp/src/impl/weights/sam_vit_h_4b8939.pth\"):\n",
    "            warnings.warn(\"sam not available, downloading...\")\n",
    "            !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights/\n",
    "        self.m_sam_checkpoint = f\"/home/michal/Documents/FIT/DP/dp/src/impl/weights/sam_vit_h_4b8939.pth\"\n",
    "        self.m_model_type = \"vit_h\"\n",
    "        self.m_device = device\n",
    "        self.sam = sam_model_registry[self.m_model_type](checkpoint=self.m_sam_checkpoint)\n",
    "        self.sam.to(device=self.m_device)\n",
    "        self.m_predictor = SamPredictor(self.sam)\n",
    "\n",
    "    def transformBoxes(self, video_dims, detections):\n",
    "        transformed_boxes = self.m_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy,\n",
    "                                                                         video_dims)\n",
    "        return transformed_boxes\n",
    "\n",
    "    def predict(self, frame, transformed_boxes):\n",
    "        self.m_predictor.set_image(frame)\n",
    "        masks, scores, logits = self.m_predictor.predict_torch(\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False,\n",
    "            point_coords=None,\n",
    "            point_labels=None\n",
    "        )\n",
    "        masks = np.array(masks.cpu())\n",
    "        return masks, scores, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d992fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:21.906132Z",
     "iopub.status.busy": "2024-02-06T20:53:21.905347Z",
     "iopub.status.idle": "2024-02-06T20:53:21.915762Z",
     "shell.execute_reply": "2024-02-06T20:53:21.914776Z"
    },
    "papermill": {
     "duration": 0.018457,
     "end_time": "2024-02-06T20:53:21.918474",
     "exception": false,
     "start_time": "2024-02-06T20:53:21.900017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f745cbee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:21.930783Z",
     "iopub.status.busy": "2024-02-06T20:53:21.930439Z",
     "iopub.status.idle": "2024-02-06T20:53:37.582902Z",
     "shell.execute_reply": "2024-02-06T20:53:37.582093Z"
    },
    "papermill": {
     "duration": 15.661219,
     "end_time": "2024-02-06T20:53:37.585295",
     "exception": false,
     "start_time": "2024-02-06T20:53:21.924076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.1.9-py3-none-any.whl.metadata (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.2/40.2 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.4)\r\n",
      "Requirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.24.4)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\r\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\r\n",
      "Collecting thop>=0.1.1 (from ultralytics)\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Downloading ultralytics-8.1.9-py3-none-any.whl (709 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m709.3/709.3 kB\u001B[0m \u001B[31m14.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: thop, ultralytics\r\n",
      "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.1.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class YOLOHandler:\n",
    "    def __init__(self, chosen_class_ids=[0]):\n",
    "        HOME ='/kaggle/working'\n",
    "        self.m_yolo_model = YOLO(f'{HOME}/weights/yolov8n.pt')\n",
    "        if not isinstance(chosen_class_ids, list):\n",
    "            raise Exception(\"Chosen class ids is not a list type.\")\n",
    "        if max(chosen_class_ids) > max(self.m_yolo_model.names.keys()):\n",
    "            raise Exception(f\"Invalid class id, max id is: {max(self.m_yolo_model.names.keys())}\")\n",
    "        if min(chosen_class_ids) < min(self.m_yolo_model.names.keys()):\n",
    "            raise Exception(f\"Invalid class id, min id is: {min(self.m_yolo_model.names.keys())}\")\n",
    "        self.chosen_class_ids = chosen_class_ids\n",
    "        self.colors = np.random.randint(0, 256, size=(len(self.m_yolo_model.names), 3))\n",
    "\n",
    "    def PrintAvailableModelNames(self):\n",
    "        print(self.m_yolo_model.names)\n",
    "\n",
    "    def set_chosenClassIds(self, chosen_class_ids):\n",
    "        self.chosen_class_ids = chosen_class_ids\n",
    "\n",
    "    def predict(self, frame):\n",
    "        detections = self.m_yolo_model.predict(frame, conf=0.7)\n",
    "        return detections\n",
    "\n",
    "    def get_color(self, color):\n",
    "        return int(color[0]), int(color[1]), int(color[2])\n",
    "\n",
    "    def visualizeDetectionsBbox(self, frame, boxes, conf_thresholds, class_ids):\n",
    "        frame_copy = np.copy(frame)\n",
    "        for idx in range(len(boxes)):\n",
    "            class_id = int(class_ids[idx])\n",
    "            conf = float(conf_thresholds[idx])\n",
    "            x1, y1, x2, y2 = int(boxes[idx][0]), int(boxes[idx][1]), int(boxes[idx][2]), int(boxes[idx][3])\n",
    "            color = self.colors[class_id]\n",
    "            label = f\"{self.m_yolo_model.names[class_id]}: {conf:.2f}\"\n",
    "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), self.get_color(color), 2)\n",
    "            cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, self.get_color(color), 2)\n",
    "        return frame_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83690a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:37.596997Z",
     "iopub.status.busy": "2024-02-06T20:53:37.596669Z",
     "iopub.status.idle": "2024-02-06T20:53:37.627438Z",
     "shell.execute_reply": "2024-02-06T20:53:37.626700Z"
    },
    "papermill": {
     "duration": 0.03906,
     "end_time": "2024-02-06T20:53:37.629431",
     "exception": false,
     "start_time": "2024-02-06T20:53:37.590371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "class VideoMTT:\n",
    "    def __init__(self, input_video=None, MTT=None, SAM=None, YOLO=None, output_video=None):\n",
    "        self.m_output_video = None\n",
    "        self.m_YOLO = None\n",
    "        self.m_SAM = None\n",
    "        self.m_MTT = None\n",
    "        self.m_input_video = None\n",
    "        self.set_inputVideo(input_video)\n",
    "        self.set_MTT(MTT)\n",
    "        self.set_SAM(SAM)\n",
    "        self.set_YOLO(YOLO)\n",
    "        self.set_outputVideo(output_video)\n",
    "        self.m_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def set_inputVideo(self, input_video):\n",
    "        if not os.path.exists(input_video):\n",
    "            raise Exception(\"Input file does not exists.\")\n",
    "        pattern = r\".*.mp4\"\n",
    "        if not re.match(pattern, input_video):\n",
    "            raise Exception(\"Input file is not mp4 format.\")\n",
    "        self.m_input_video = input_video\n",
    "\n",
    "    def set_MTT(self, MTT):\n",
    "        self.m_MTT = MTT\n",
    "\n",
    "    def set_SAM(self, SAM):\n",
    "        self.m_SAM = SAM\n",
    "\n",
    "    def set_YOLO(self, YOLO):\n",
    "        self.m_YOLO = YOLO\n",
    "\n",
    "    def set_outputVideo(self, output_video):\n",
    "        self.m_output_video = output_video\n",
    "\n",
    "    def set_device(self, device):\n",
    "        if device != \"cpu\" and not torch.cuda.is_available():\n",
    "            warnings.warn(\"GPU is not available, setting device to cpu.\")\n",
    "            self.m_device = \"cpu\"\n",
    "            return\n",
    "        self.m_device = \"cpu\"\n",
    "        return\n",
    "\n",
    "    def checkClassMembers(self):\n",
    "        if self.m_MTT == None:\n",
    "            raise Exception(\"MTT is not set\")\n",
    "        if not isinstance(self.m_SAM, SAM_handler):\n",
    "            raise Exception(\"SAM is not set\")\n",
    "        if not isinstance(self.m_YOLO, YOLOHandler):\n",
    "            raise Exception(\"YOLO is not set\")\n",
    "        if self.m_output_video == None:\n",
    "            raise Exception(\"Output video is not set\")\n",
    "        if self.m_input_video == None:\n",
    "            raise Exception(\"Input video is not set\")\n",
    "\n",
    "    def get_videoDimensions(self, cap):\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        return width, height\n",
    "\n",
    "    def get_videoFps(self, cap):\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        return fps\n",
    "\n",
    "    def get_outputVideoWriter(self, input_cap, output_path):\n",
    "        # Get the video's properties (width, height, FPS)\n",
    "        width, height = self.get_videoDimensions(input_cap)\n",
    "        fps = self.get_videoFps(input_cap)\n",
    "\n",
    "        # Define the output video file\n",
    "        output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
    "        output_video = cv2.VideoWriter(output_path, output_codec, fps, (width, height))\n",
    "\n",
    "        return output_video\n",
    "    \n",
    "    def get_color(self, color):\n",
    "        return int(color[0]), int(color[1]), int(color[2])\n",
    "\n",
    "    def add_color(self, mask, color):\n",
    "        next_mask = mask.astype(np.uint8)\n",
    "        next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
    "        next_mask = np.moveaxis(next_mask, 0, -1)\n",
    "        return next_mask * color\n",
    "\n",
    "# Merge masks into a single, multi-colored mask\n",
    "    def merge_masks_colored(self, masks, class_ids):\n",
    "        filtered_class_ids = []\n",
    "        filtered_masks = []\n",
    "        for idx, cid in enumerate(class_ids):\n",
    "            if int(cid) in self.m_YOLO.chosen_class_ids:\n",
    "                filtered_class_ids.append(cid)\n",
    "                filtered_masks.append(masks[idx])\n",
    "\n",
    "        merged_with_colors = self.add_color(filtered_masks[0][0], self.get_color(self.m_YOLO.colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
    "\n",
    "        if len(filtered_masks) == 1:\n",
    "            return merged_with_colors\n",
    "\n",
    "        for i in range(1, len(filtered_masks)):\n",
    "            curr_mask_with_colors = self.add_color(filtered_masks[i][0], self.get_color(self.m_YOLO.colors[int(filtered_class_ids[i])]))\n",
    "            merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n",
    "        print(\"merged with colors shape: \", merged_with_colors.shape)\n",
    "        return merged_with_colors.astype(np.uint8)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.checkClassMembers()\n",
    "        videoCap = cv2.VideoCapture(self.m_input_video)\n",
    "        output_video_boxes = self.get_outputVideoWriter(videoCap, self.m_output_video + \"_boxes.mp4\")\n",
    "        output_video_masks = self.get_outputVideoWriter(videoCap, self.m_output_video + \"_masks.mp4\")\n",
    "        # print(self.m_input_video)\n",
    "        frame_num = 1\n",
    "        while videoCap.isOpened():\n",
    "            print(\"frame: \", frame_num)\n",
    "            ret, frame = videoCap.read()\n",
    "            if frame_num < 50:\n",
    "                frame_num +=1\n",
    "                continue\n",
    "            if not ret:\n",
    "                break\n",
    "            yoloDetections = self.m_YOLO.predict(frame)\n",
    "            frameWithYoloDetections = self.m_YOLO.visualizeDetectionsBbox(frame,\n",
    "                                                                          yoloDetections[0].boxes.cpu().xyxy,\n",
    "                                                                          yoloDetections[0].boxes.cpu().conf,\n",
    "                                                                          yoloDetections[0].boxes.cpu().cls)\n",
    "            output_video_boxes.write(frameWithYoloDetections)\n",
    "            # output_video_boxes.write(frame)\n",
    "            print(\"frame with yolo detections shape: \", frameWithYoloDetections.shape)\n",
    "            transformedBoxes = self.m_SAM.transformBoxes(detections=yoloDetections,\n",
    "                                                         video_dims=list(self.get_videoDimensions(videoCap)))\n",
    "            print(\"transformed boxes shape: \", transformedBoxes.shape)\n",
    "\n",
    "            if len(transformedBoxes) == 0:\n",
    "                print(\"No boxes found on frame\", frame_num)\n",
    "                output_video_masks.write(frame)\n",
    "                frame_num += 1\n",
    "                continue\n",
    "            masks, scores, logits = self.m_SAM.predict(frame, transformedBoxes)\n",
    "            print(\"masks shape: \", masks.shape)\n",
    "            if masks is None or len(masks) == 0:\n",
    "                print(\"No masks found on frame\", frame_num)\n",
    "                output_video_masks.write(frame)\n",
    "                frame_num += 1\n",
    "                continue\n",
    "                \n",
    "            merged_colored_mask = self.merge_masks_colored(masks, yoloDetections[0].boxes.cls)\n",
    "  \n",
    "          # Write masks to output video\n",
    "            image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
    "            output_video_masks.write(image_combined)\n",
    "            \n",
    "            \n",
    "            frame_num += 1\n",
    "            if frame_num > 55:\n",
    "                break\n",
    "        videoCap.release()\n",
    "        output_video_boxes.release()\n",
    "        output_video_masks.release()\n",
    "#         cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28606ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:53:37.640330Z",
     "iopub.status.busy": "2024-02-06T20:53:37.640072Z",
     "iopub.status.idle": "2024-02-06T20:54:09.317019Z",
     "shell.execute_reply": "2024-02-06T20:54:09.315934Z"
    },
    "papermill": {
     "duration": 31.685018,
     "end_time": "2024-02-06T20:54:09.319481",
     "exception": false,
     "start_time": "2024-02-06T20:53:37.634463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to '/kaggle/working/weights/yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 75.8MB/s]\n",
      "/tmp/ipykernel_25/3848539943.py:13: UserWarning: sam not available, downloading...\n",
      "  warnings.warn(\"sam not available, downloading...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "frame:  1\n",
      "frame:  2\n",
      "frame:  3\n",
      "frame:  4\n",
      "frame:  5\n",
      "frame:  6\n",
      "frame:  7\n",
      "frame:  8\n",
      "frame:  9\n",
      "frame:  10\n",
      "frame:  11\n",
      "frame:  12\n",
      "frame:  13\n",
      "frame:  14\n",
      "frame:  15\n",
      "frame:  16\n",
      "frame:  17\n",
      "frame:  18\n",
      "frame:  19\n",
      "frame:  20\n",
      "frame:  21\n",
      "frame:  22\n",
      "frame:  23\n",
      "frame:  24\n",
      "frame:  25\n",
      "frame:  26\n",
      "frame:  27\n",
      "frame:  28\n",
      "frame:  29\n",
      "frame:  30\n",
      "frame:  31\n",
      "frame:  32\n",
      "frame:  33\n",
      "frame:  34\n",
      "frame:  35\n",
      "frame:  36\n",
      "frame:  37\n",
      "frame:  38\n",
      "frame:  39\n",
      "frame:  40\n",
      "frame:  41\n",
      "frame:  42\n",
      "frame:  43\n",
      "frame:  44\n",
      "frame:  45\n",
      "frame:  46\n",
      "frame:  47\n",
      "frame:  48\n",
      "frame:  49\n",
      "frame:  50\n",
      "\n",
      "0: 384x640 8 persons, 1 bird, 128.2ms\n",
      "Speed: 12.6ms preprocess, 128.2ms inference, 628.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([9, 4])\n",
      "masks shape:  (9, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n",
      "frame:  51\n",
      "\n",
      "0: 384x640 8 persons, 1 bird, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([9, 4])\n",
      "masks shape:  (9, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n",
      "frame:  52\n",
      "\n",
      "0: 384x640 9 persons, 1 bird, 8.7ms\n",
      "Speed: 2.5ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([10, 4])\n",
      "masks shape:  (10, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n",
      "frame:  53\n",
      "\n",
      "0: 384x640 6 persons, 1 bird, 8.6ms\n",
      "Speed: 2.6ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([7, 4])\n",
      "masks shape:  (7, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n",
      "frame:  54\n",
      "\n",
      "0: 384x640 7 persons, 1 bird, 8.0ms\n",
      "Speed: 2.5ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([8, 4])\n",
      "masks shape:  (8, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n",
      "frame:  55\n",
      "\n",
      "0: 384x640 8 persons, 1 bird, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame with yolo detections shape:  (720, 1280, 3)\n",
      "transformed boxes shape:  torch.Size([9, 4])\n",
      "masks shape:  (9, 1, 720, 1280)\n",
      "merged with colors shape:  (720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "!mkdir output\n",
    "of = f\"{HOME}/output/people\"\n",
    "yolo = YOLOHandler()\n",
    "mtt = \"XXX\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: \", device)\n",
    "sam = SAM_handler(device = device)\n",
    "input = \"/kaggle/input/input-data/people_01.mp4\"\n",
    "vid = VideoMTT(input_video=input, MTT = mtt, SAM=sam, YOLO=yolo, output_video=of)\n",
    "vid.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68edea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T20:54:09.335840Z",
     "iopub.status.busy": "2024-02-06T20:54:09.335497Z",
     "iopub.status.idle": "2024-02-06T20:54:09.341890Z",
     "shell.execute_reply": "2024-02-06T20:54:09.340987Z"
    },
    "papermill": {
     "duration": 0.016812,
     "end_time": "2024-02-06T20:54:09.343695",
     "exception": false,
     "start_time": "2024-02-06T20:54:09.326883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33ab7f",
   "metadata": {
    "papermill": {
     "duration": 0.006945,
     "end_time": "2024-02-06T20:54:09.357786",
     "exception": false,
     "start_time": "2024-02-06T20:54:09.350841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4373492,
     "sourceId": 7513474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.918541,
   "end_time": "2024-02-06T20:54:11.088652",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-06T20:53:10.170111",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
